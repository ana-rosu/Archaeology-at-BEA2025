{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "MODEL_NAME = \"gpt2-xl\"\n",
        "NUM_CLASSES = 3\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LR = 2e-5\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "train_df = pd.read_csv(\"mistake_identification_train.csv\")\n",
        "val_df = pd.read_csv(\"mistake_identification_val.csv\")\n",
        "class PedagogyDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        df = df[df['mistake_identification'].isin([0, 1, 2])].copy()\n",
        "        self.texts = df[\"response\"].tolist()\n",
        "        self.labels = df[\"mistake_identification\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encodings = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encodings[\"input_ids\"].squeeze(0),\n",
        "            'attention_mask': encodings[\"attention_mask\"].squeeze(0),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = PedagogyDataset(train_df, tokenizer)\n",
        "val_dataset = PedagogyDataset(val_df, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "class DecoderClassifier(nn.Module):\n",
        "    def __init__(self, model_name=MODEL_NAME, num_classes=3, dropout=0.1):\n",
        "        super(DecoderClassifier, self).__init__()\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        hidden_size = self.transformer.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        device = self.classifier.weight.device\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        outputs = self.transformer(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.last_hidden_state  # shape (B, T, H)\n",
        "        last_token_index = (input_ids != tokenizer.pad_token_id).sum(dim=1) - 1\n",
        "        batch_size = input_ids.size(0)\n",
        "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size())\n",
        "        masked_hidden = hidden_states * attention_mask_expanded\n",
        "        pooled_output = masked_hidden.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = DecoderClassifier(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "scaler = GradScaler()\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].cuda()\n",
        "            attention_mask = batch['attention_mask'].cuda()\n",
        "            labels = batch['labels'].cuda()\n",
        "            with autocast():\n",
        "                logits = model(input_ids, attention_mask)\n",
        "                labels = labels.to(logits.device)\n",
        "                loss = loss_fn(logits, labels)\n",
        "                val_running_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "    val_loss = val_running_loss / len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    return acc, f1, val_loss\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].cuda()\n",
        "        attention_mask = batch['attention_mask'].cuda()\n",
        "        labels = batch['labels'].cuda()\n",
        "\n",
        "        with autocast():\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            labels = labels.to(logits.device)  # ensure alignment\n",
        "            loss = loss_fn(logits, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        train_loss = running_loss/len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "    acc, f1, val_loss = evaluate(model, val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrowVS3rP3nV",
        "outputId": "70343acb-02b6-4d65-8026-145ef25d3b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5221b4bff43f>:105: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-4-5221b4bff43f>:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-4-5221b4bff43f>:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train Loss: 0.6845 | Val Loss: 0.5731 | Acc: 0.8499 | F1: 0.5017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5221b4bff43f>:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-4-5221b4bff43f>:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 | Train Loss: 0.5657 | Val Loss: 0.5291 | Acc: 0.8357 | F1: 0.5106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5221b4bff43f>:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-4-5221b4bff43f>:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 | Train Loss: 0.5155 | Val Loss: 0.4853 | Acc: 0.8479 | F1: 0.5222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5221b4bff43f>:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-4-5221b4bff43f>:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 | Train Loss: 0.4520 | Val Loss: 0.4460 | Acc: 0.8621 | F1: 0.6251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5221b4bff43f>:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-4-5221b4bff43f>:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 | Train Loss: 0.4000 | Val Loss: 0.4816 | Acc: 0.8580 | F1: 0.6341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zKBJNKz2sbYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def plot_conf_matrix(model, dataloader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].cuda()\n",
        "            attention_mask = batch['attention_mask'].cuda()\n",
        "            labels = batch['labels'].cuda()\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"No\", \"To some extent\", \"Yes\"])\n",
        "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "    plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "    plt.show()\n",
        "\n",
        "plot_conf_matrix(model, val_loader)"
      ],
      "metadata": {
        "id": "jydLQC5ysdeP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}